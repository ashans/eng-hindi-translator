{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK WITH ATTENTION**   \n",
        "> Sequence to Sequence network for translating English to Hindi and Hindi to English\n",
        "\n",
        "This code contains two parts  \n",
        "  - Preprocessing dataset, training two networks and save models (In `train_and_save()` funtion)\n",
        "  - Load saved models with trimmed dataset and evaluate random examples (In `sample_evaluate()` function)\n",
        "*******************************************************\n",
        "**Files**\n",
        " - Training/Testing dataset : [trimmed_data.csv](https://raw.githubusercontent.com/ashans/eng-hindi-translator/main/output/trimmed_data.csv)\n",
        " - Pickled files\n",
        "   - English to Hindi Encoder : [eng_to_hin_encoder.pkl](https://raw.githubusercontent.com/ashans/eng-hindi-translator/main/output/eng_to_hin_encoder.pkl)\n",
        "   - English to Hindi Decoder : [eng_to_hin_decoder.pkl](https://raw.githubusercontent.com/ashans/eng-hindi-translator/main/output/eng_to_hin_decoder.pkl)\n",
        "   - Hindi to English Encoder : [hin_to_eng_encoder.pkl](https://raw.githubusercontent.com/ashans/eng-hindi-translator/main/output/hin_to_eng_encoder.pkl)\n",
        "   - Hindi to English Decoder : [hin_to_eng_decoder.pkl](https://raw.githubusercontent.com/ashans/eng-hindi-translator/main/output/hin_to_eng_decoder.pkl)\n",
        "\n",
        "**Pipeline method**\n",
        "- `sample_evaluate()`\n",
        "*******************************************************\n",
        "*Original Dataset - [https://www.manythings.org/anki/hin-eng.zip](https://www.manythings.org/anki/hin-eng.zip)*  \n",
        "*Code Reference - [NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)*  \n",
        "*Github - [https://github.com/ashans/eng-hindi-translator](https://github.com/ashans/eng-hindi-translator)*"
      ],
      "metadata": {
        "id": "oOtcVWInDh_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Requirements**"
      ],
      "metadata": {
        "id": "-PaU7CjRFuRb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwCa1bIHdKVH",
        "outputId": "b5af4212-308e-4567-ff4b-c9bf12adf6dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using - cpu\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import urllib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import model_zoo\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using - %s\" %device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating helper class for `word2index` and `index2word`"
      ],
      "metadata": {
        "id": "z3B2e-B9F-0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YItOPyM9ddPJ"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitter and character replacing"
      ],
      "metadata": {
        "id": "WOIwL3YeGMw7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BNphbvfddgN3"
      },
      "outputs": [],
      "source": [
        "def splitNormalize(s):\n",
        "      splitted = s.split('\\t')[:2]\n",
        "      splitted[0] = re.sub(r\"([.!?])\", r\" \\1\", splitted[0].lower().strip())\n",
        "      splitted[0] = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", splitted[0])\n",
        "\n",
        "      splitted[1] = re.sub(r\"([.!?ред])\", r\" \\1\", splitted[1].strip())\n",
        "\n",
        "      return splitted[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading dataset"
      ],
      "metadata": {
        "id": "AnD_81gUGRsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eF2jlyz0djyX"
      },
      "outputs": [],
      "source": [
        "DATA_URL = \"https://raw.githubusercontent.com/ashans/eng-hindi-translator/main/dataset/hin.txt\"\n",
        "def load_content():\n",
        "  file = urllib.request.urlopen(DATA_URL)\n",
        "  \n",
        "  s = []\n",
        "  for line in file:\n",
        "    s.append(splitNormalize(line.decode(\"utf-8\")))\n",
        "  print(\"Loaded sample data - %s\" % s[0])\n",
        "  return s"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering samples from dataset\n",
        "> Only samples with limited number of words and types are selected"
      ],
      "metadata": {
        "id": "AspN0K01GWIK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k6Pl5O29g6cd"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k6Bw-cFphAwV"
      },
      "outputs": [],
      "source": [
        "def prepareData():\n",
        "    pairs = load_content()\n",
        "    eng_lang = Lang('eng')\n",
        "    hin_lang = Lang('hin')\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Sample data - %s\" % pairs[0])\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        eng_lang.addSentence(pair[0])\n",
        "        hin_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(eng_lang.name, eng_lang.n_words)\n",
        "    print(hin_lang.name, hin_lang.n_words)\n",
        "    return eng_lang, hin_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "YlJH9LmrGlvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r1MpMDXtj_fD"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder with Attention"
      ],
      "metadata": {
        "id": "GArVCi_qGoDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mPsm1FWNkDxy"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JANi9XwUkKuq"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair, input_lang, output_lang):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training function with **teacher forcing**"
      ],
      "metadata": {
        "id": "e9_Iex_OG4Jp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k9DIwZ5smglB"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper to print elapsed time"
      ],
      "metadata": {
        "id": "nJ19dU_NJh3I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NVnXKFBvmmIX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss."
      ],
      "metadata": {
        "id": "kkFjXrkTJp5F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V6SJNtWpmoH_"
      },
      "outputs": [],
      "source": [
        "def trainIters(input_lang, output_lang, pairs, encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs), input_lang, output_lang)\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ymkpK33XnAgg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation function"
      ],
      "metadata": {
        "id": "nrUVTi0cJvRw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2RnLyRJWnEjn"
      },
      "outputs": [],
      "source": [
        "def evaluate(input_lang, output_lang, encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                # decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rc9cOxDnqP-5"
      },
      "outputs": [],
      "source": [
        "def save_model(model, name):\n",
        "    torch.save(model, f'{name}.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RCTKjPzesMhI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "def save_data(data, name):\n",
        "    # open the file in the write mode\n",
        "    with open(f'{name}.csv', 'w') as f:\n",
        "        # create the csv writer\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        # write a row to the csv file\n",
        "        for line in data:\n",
        "            writer.writerow(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the models and save outputs**"
      ],
      "metadata": {
        "id": "bAtBwMHGJzuX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UveIrtWFhFUN"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "def train_and_save():  \n",
        "    eng_lang, hin_lang, eng_hindi_pairs = prepareData()\n",
        "\n",
        "    print(\"Saving trimmed data\")\n",
        "    save_data(eng_hindi_pairs, \"trimmed_data\")\n",
        "\n",
        "    # Train English to Hindi translation\n",
        "    print(\"Training English to Hindi translation\")\n",
        "    encoder1 = EncoderRNN(eng_lang.n_words, hidden_size).to(device)\n",
        "    attn_decoder1 = AttnDecoderRNN(hidden_size, hin_lang.n_words, dropout_p=0.1).to(device)\n",
        "    trainIters(eng_lang, hin_lang, eng_hindi_pairs, encoder1, attn_decoder1, 15000, print_every=5000)\n",
        "\n",
        "    # Save English to Hindi translation models\n",
        "    print(\"Saving English to Hindi translation models\")\n",
        "    save_model(encoder1, 'eng_to_hin_encoder')\n",
        "    save_model(attn_decoder1, 'eng_to_hin_decoder')\n",
        "\n",
        "    # Train Hindi to English translation\n",
        "    print(\"Training Hindi to English translation\")\n",
        "    encoder2 = EncoderRNN(hin_lang.n_words, hidden_size).to(device)\n",
        "    attn_decoder2 = AttnDecoderRNN(hidden_size, eng_lang.n_words, dropout_p=0.1).to(device)\n",
        "    trainIters(hin_lang,eng_lang, [list(reversed(p)) for p in eng_hindi_pairs], encoder2, attn_decoder2, 15000, print_every=5000)\n",
        "\n",
        "    # Save Hindi to English translation models\n",
        "    print(\"Saving Hindi to English translation models\")\n",
        "    save_model(encoder2, 'hin_to_eng_encoder')\n",
        "    save_model(attn_decoder2, 'hin_to_eng_decoder')\n",
        "\n",
        "    print(\"Complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GwZMnjKCBSDp"
      },
      "outputs": [],
      "source": [
        "def load_model(url):\n",
        "  model = model_zoo.load_url(f'https://raw.githubusercontent.com/ashans/eng-hindi-translator/main/output/{url}')\n",
        "  # model = torch.load(url)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6pJRPVx9IxDM"
      },
      "outputs": [],
      "source": [
        "def load_trimmed_data():\n",
        "  file = urllib.request.urlopen('https://raw.githubusercontent.com/ashans/eng-hindi-translator/main/output/trimmed_data.csv')\n",
        "  \n",
        "  s = []\n",
        "  for line in file:\n",
        "    s.append(line.decode(\"utf-8\").strip().split(','))\n",
        "\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0KWworVTHrwE"
      },
      "outputs": [],
      "source": [
        "def load_models():\n",
        "  eng_to_hin_encoder = load_model('eng_to_hin_encoder.pkl')\n",
        "  eng_to_hin_decoder = load_model('eng_to_hin_decoder.pkl')\n",
        "  hin_to_eng_encoder = load_model('hin_to_eng_encoder.pkl')\n",
        "  hin_to_eng_decoder = load_model('hin_to_eng_decoder.pkl')\n",
        "\n",
        "  return eng_to_hin_encoder, eng_to_hin_decoder, hin_to_eng_encoder, hin_to_eng_decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YAB9wi_CPR8P"
      },
      "outputs": [],
      "source": [
        "def translate(input_lang, output_lang, encoder, decoder, sentence):\n",
        "  output_words, _ = evaluate(input_lang, output_lang, encoder, decoder, sentence)\n",
        "  return ' '.join(output_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "h2yU1kSnS8FV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def show_table(data):\n",
        "  table = pd.DataFrame(data, columns = ['Original English', 'Original Hindi', 'Engish -> Hindi', 'English -> Hindi -> English'])\n",
        "\n",
        "  display(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load `pkl` files to models and evaluate given number of samples**"
      ],
      "metadata": {
        "id": "kW14Kr1jJ6v_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fhnxtj9-IaBL"
      },
      "outputs": [],
      "source": [
        "def sample_evaluate(no_of_samples = 10):\n",
        "  e_to_h_encoder, e_to_h_decoder, h_to_e_encoder, h_to_e_decoder = load_models()\n",
        "  trimmed_data = load_trimmed_data()\n",
        "  eng_lang, hin_lang, _ = prepareData()\n",
        "  result = []\n",
        "  for i in range(no_of_samples):\n",
        "    selected = random.choice(trimmed_data)\n",
        "    hindi_sentence = translate(eng_lang, hin_lang, e_to_h_encoder, e_to_h_decoder, selected[0])\n",
        "    english_sentence = translate(hin_lang, eng_lang, h_to_e_encoder, h_to_e_decoder, hindi_sentence)\n",
        "    result.append([selected[0], selected[1], hindi_sentence, english_sentence])\n",
        "  print('---------------- Results -----------------')\n",
        "  show_table(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_and_save() # Uncomment to train two pairs of encoder/decoder and save trimmed dataset and pkl files"
      ],
      "metadata": {
        "id": "OUqvfp2eDFct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "b4jnE90aK6-q",
        "outputId": "7780bbf0-4ae4-4ee4-fd9c-1503579f5fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded sample data - ['wow !', 'рд╡рд╛рд╣ !']\n",
            "Read 2952 sentence pairs\n",
            "Sample data - ['wow !', 'рд╡рд╛рд╣ !']\n",
            "Trimmed to 179 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 293\n",
            "hin 338\n",
            "---------------- Results -----------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3d6cced4-26b1-4ea6-b588-7248a9350c06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original English</th>\n",
              "      <th>Original Hindi</th>\n",
              "      <th>Engish -&gt; Hindi</th>\n",
              "      <th>English -&gt; Hindi -&gt; English</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i m disappointed with you .</td>\n",
              "      <td>рдореИрдВ рддреБрдорд╕реЗ рдирд┐рд░рд╛рд╢ рд╣реВрдБ ред</td>\n",
              "      <td>рдореИрдВ рддреБрдорд╕реЗ рдирд┐рд░рд╛рд╢ рд╣реВрдБ ред</td>\n",
              "      <td>i m disappointed with you .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i m not very old .</td>\n",
              "      <td>рдореИрдВ рдмрд╣реБрдд рдмреВрдврд╝реА рдирд╣реАрдВ рд╣реВрдБ  ред</td>\n",
              "      <td>рдореИрдВ рдмрд╣реБрдд рдмреВрдврд╝реА рдирд╣реАрдВ рд╣реВрдБ  ред</td>\n",
              "      <td>i m not very old .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i m busy with my homework .</td>\n",
              "      <td>рдореИрдВ рдЕрдкрдиреЗ рд╣реЛрдорд╡рд░реНрдХ рдореЗрдВ рд▓рдЧрд╛ рд╣реБрдЖ рд╣реВрдБ ред</td>\n",
              "      <td>рдореИрдВ рдЕрдкрдиреЗ рд╣реЛрдорд╡рд░реНрдХ рдореЗрдВ рд▓рдЧрд╛ рд╣реБрдЖ рд╣реВрдБ ред</td>\n",
              "      <td>i m busy with my homework .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>she is poor but happy .</td>\n",
              "      <td>\"рд╡рд╣ рдЧрд░реАрдм рд╣реИ</td>\n",
              "      <td>рд╡рд╣ рдЧрд░реАрдм рд╣реИ, рдкрд░ рдЦреБрд╢ рд╣реИ ред</td>\n",
              "      <td>he is poor but happy .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i am confident he will keep his promise .</td>\n",
              "      <td>рдореБрдЭреЗ рднрд░реЛрд╕рд╛ рд╣реИ рдХрд┐ рд╡рд╣ рдЕрдкрдирд╛ рд╡рд╛рджрд╛ рдирд┐рднрд╛рдПрдЧрд╛ ред</td>\n",
              "      <td>рдореБрдЭреЗ рднрд░реЛрд╕рд╛ рд╣реИ рдХрд┐ рд╡рд╣ рдЕрдкрдирд╛ рд╡рд╛рджрд╛ рдирд┐рднрд╛рдПрдЧрд╛ ред</td>\n",
              "      <td>i am confident he will keep his promise .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>i m going to tell you something .</td>\n",
              "      <td>рдореИрдВ рддреБрдореНрд╣реЗрдВ рдХреБрдЫ рдмрддрд╛рдиреЗ рдЬрд╛ рд░рд╣рд╛ рд╣реВрдБ ред</td>\n",
              "      <td>рдореИрдВ рддреБрдореНрд╣реЗрдВ рдХреБрдЫ рдмрддрд╛рдиреЗ рд╡рд╛рд▓рд╛ рд╣реВрдБ ред</td>\n",
              "      <td>i m going to tell you something .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>i am his friend and will remain so .</td>\n",
              "      <td>рдореИрдВ рдЙрд╕рдХрд╛ рджреЛрд╕реНрдд рд╣реВрдБ рдФрд░ рд░рд╣реВрдБрдЧрд╛ ред</td>\n",
              "      <td>рдореИрдВ рдЙрд╕рдХрд╛ рджреЛрд╕реНрдд рд╣реВрдБ рдФрд░ рд░рд╣реВрдБрдЧрд╛ ред</td>\n",
              "      <td>i am his friend and will remain so .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>we re against war .</td>\n",
              "      <td>рд╣рдо рдЬрдВрдЧ рдХреЗ рдЦрд╝рд┐рд▓рд╛рдлрд╝ рд╣реИрдВ ред</td>\n",
              "      <td>рд╣рдо рдпреБрджреНрдз рдХрд╛ рд╡рд┐рд░реЛрдз рдХрд░рддреЗ рд╣реИрдВ ред</td>\n",
              "      <td>we re against war .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>she is not afraid of anything .</td>\n",
              "      <td>рд╡рд╣ рдХрд┐рд╕реА рднреА рдЪреАрдЬрд╝ рд╕реЗ рдирд╣реАрдВ рдбрд░рддреА рд╣реИ ред</td>\n",
              "      <td>рд╡рд╣ рдХрд┐рд╕реА рднреА рдЪреАрдЬрд╝ рд╕реЗ рдирд╣реАрдВ рдбрд░рддреА рд╣реИ ред</td>\n",
              "      <td>she is not afraid of anything .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>she is a wealthy woman .</td>\n",
              "      <td>рд╡рд╣ рдмрд╣реБрдд рдкреИрд╕реЗрд╡рд╛рд▓реА рдФрд░рдд рд╣реИ ред</td>\n",
              "      <td>рд╡рд╣ рдмрд╣реБрдд рдкреИрд╕реЗрд╡рд╛рд▓реА рдФрд░рдд рд╣реИ ред</td>\n",
              "      <td>she is a wealthy woman .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d6cced4-26b1-4ea6-b588-7248a9350c06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d6cced4-26b1-4ea6-b588-7248a9350c06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d6cced4-26b1-4ea6-b588-7248a9350c06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                            Original English  ...                English -> Hindi -> English\n",
              "0                i m disappointed with you .  ...                i m disappointed with you .\n",
              "1                         i m not very old .  ...                         i m not very old .\n",
              "2                i m busy with my homework .  ...                i m busy with my homework .\n",
              "3                    she is poor but happy .  ...                     he is poor but happy .\n",
              "4  i am confident he will keep his promise .  ...  i am confident he will keep his promise .\n",
              "5          i m going to tell you something .  ...          i m going to tell you something .\n",
              "6       i am his friend and will remain so .  ...       i am his friend and will remain so .\n",
              "7                        we re against war .  ...                        we re against war .\n",
              "8            she is not afraid of anything .  ...            she is not afraid of anything .\n",
              "9                   she is a wealthy woman .  ...                   she is a wealthy woman .\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# This will load the pkl files and trimmed dataset from github and evaluate provided number of random examples\n",
        "sample_evaluate(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "two_way_seq2seq_translator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}